# HarmonySafeAgent Configuration Template
# Copy this file to ~/.harmony-agent/config.yml and customize

ai:
  provider: openai  # openai | deepseek | claude
  api_key: ${OPENAI_API_KEY}  # Use environment variable or set directly
  model: gpt-4-turbo
  max_tokens: 4096
  temperature: 0.3
  base_url: https://api.openai.com/v1

  # Multiple Provider Support (Phase 3)
  providers:
    openai:
      api_key: ${OPENAI_API_KEY}
      base_url: https://api.openai.com/v1
      models:
        fast: gpt-3.5-turbo
        standard: gpt-4-turbo
        premium: gpt-4

    claude:
      api_key: ${CLAUDE_API_KEY}
      base_url: https://api.anthropic.com/v1
      models:
        fast: claude-3-haiku-20240307
        standard: claude-3-sonnet-20240229
        premium: claude-3-opus-20240229

    siliconflow:
      api_key: 
      base_url: https://api.siliconflow.cn/v1
      models:
        fast: Qwen/Qwen2.5-7B-Instruct
        standard: Qwen/Qwen2.5-14B-Instruct
        premium: Qwen/Qwen2.5-72B-Instruct
        coder: Qwen/Qwen2.5-Coder-7B-Instruct

  # Role-based Model Selection
  roles:
    analyzer:
      provider: siliconflow
      model: fast  # Use fast model for quick analysis
      temperature: 0.3
      max_tokens: 1000

    planner:
      provider: siliconflow
      model: standard  # Use standard model for design
      temperature: 0.5
      max_tokens: 2500

    coder:
      provider: siliconflow
      model: coder  # Use Qwen Coder for code generation
      temperature: 0.2
      max_tokens: 3000

    reviewer:
      provider: siliconflow
      model: standard  # Use standard model for thorough review
      temperature: 0.7
      max_tokens: 2500

    tester:
      provider: siliconflow
      model: standard  # Use standard model for validation and testing
      temperature: 0.3
      max_tokens: 2000

  # Command-based Model Selection (for CLI commands)
  commands:
    refactor:
      provider: siliconflow
      model: coder  # Use Qwen Coder for Rust migration advice
      temperature: 0.3
      max_tokens: 4096

    suggest:
      provider: siliconflow
      model: standard  # Use standard model for security suggestions
      temperature: 0.5
      max_tokens: 3000

analysis:
  level: standard  # quick | standard | deep
  parallel: true
  max_threads: 4
  incremental: false
  timeout: 300  # seconds

tools:
  clang_path: clang
  semgrep_path: semgrep
  rust_path: rustc

output:
  format: html  # markdown | html | json
  verbose: true
  color: true

cache:
  enabled: true
  ttl: 3600  # seconds
  max_size: 100  # MB
