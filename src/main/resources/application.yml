# HarmonySafeAgent Configuration Template
# Copy this file to ~/.harmony-agent/config.yml and customize

ai:
  provider: siliconflow  # openai | deepseek | claude | siliconflow
  api_key: ${SILICONFLOW_API_KEY}  # Use environment variable or set directly
  model: Qwen/Qwen2.5-14B-Instruct
  max_tokens: 4096
  temperature: 0.3
  base_url: https://api.siliconflow.cn/v1

  # Rate Limiting Configuration
  # Two modes: "qps" (Queries Per Second) or "tpm" (Tokens Per Minute)
  rate_limit_mode: qps  # "qps" for simple request limiting, "tpm" for token-based limiting

  # QPS Mode Configuration (simple and recommended for most use cases)
  requests_per_second_limit: 5.0  # Max API requests per second (↑ optimized from 2.0)

  # TPM Mode Configuration (more precise but requires token estimation)
  tokens_per_minute_limit: 60000  # Max tokens per minute (e.g., 60000 for OpenAI Tier 1)

  # Safety Margin (applies to both modes)
  safety_margin: 0.8  # Use 80% of the limit to handle fluctuations (↑ optimized from 0.6)

  # Concurrency Control
  validation_concurrency: 4  # Max concurrent AI validations in DecisionEngine (↑ optimized from 1)

  # Multiple Provider Support (Phase 3)
  providers:
    openai:
      api_key: sk-QrVG8lxmZUvhp0UZS3cqkujFJF25ux4QUzN1DmKk0RGjsQln
      base_url: https://api.kkyyxx.xyz
      models:
        fast: gpt-3.5-turbo
        standard: gpt-4o
        premium: gpt-4

    claude:
      api_key: ${CLAUDE_API_KEY}
      base_url: https://api.anthropic.com/v1
      models:
        fast: claude-3-haiku-20240307
        standard: claude-3-sonnet-20240229
        premium: claude-3-opus-20240229

    siliconflow:
      api_key: sk-rhtdvakaxhecyocmsznvgepyelubdpqndumpjekludfvzbxh
      base_url: https://api.siliconflow.cn/v1
      models:
        fast: Qwen/Qwen2.5-7B-Instruct
        standard: Qwen/Qwen2.5-14B-Instruct
        premium: Qwen/Qwen2.5-72B-Instruct
        coder: Qwen/Qwen2.5-Coder-7B-Instruct

  # Role-based Model Selection
  roles:
    analyzer:
      provider: siliconflow
      model: fast  # Use fast model for quick analysis
      temperature: 0.3
      max_tokens: 1000

    planner:
      provider: siliconflow
      model: standard  # Use standard model for design
      temperature: 0.5
      max_tokens: 2500

    coder:
      provider: siliconflow
      model: coder  # Use SiliconFlow Coder for code generation
      temperature: 0.2
      max_tokens: 3000

    reviewer:
      provider: siliconflow
      model: standard  # Use standard model for thorough review
      temperature: 0.7
      max_tokens: 2500

    tester:
      provider: siliconflow
      model: standard  # Use standard model for validation and testing
      temperature: 0.3
      max_tokens: 2000

  # Command-based Model Selection (for CLI commands)
  commands:
    refactor:
      provider: siliconflow
      model: coder  # Use Qwen Coder for Rust migration advice
      temperature: 0.3
      max_tokens: 4096

    suggest:
      provider: siliconflow
      model: standard  # Use standard model for security suggestions
      temperature: 0.5
      max_tokens: 3000

analysis:
  level: standard  # quick | standard | deep
  parallel: true
  max_threads: 4  # ✓ Aligned with validation_concurrency (both = 4)
  incremental: false
  timeout: 300  # seconds

tools:
  clang_path: clang
  semgrep_path: semgrep
  rust_path: rustc

output:
  format: html  # markdown | html | json
  verbose: true
  color: true
  command_history_size: 10  # Number of commands to keep in history (up/down navigation)

cache:
  enabled: true
  ttl: 3600  # seconds
  max_size: 100  # MB
